summary(modelBCLCC)
# Multicollinearity
vif(modelBCLCA)
vif(modelBCLCB)
vif(modelBCLCC)
# Autocorrelation
durbinWatsonTest(modelBCLCA)
durbinWatsonTest(modelBCLCB)
durbinWatsonTest(modelBCLCC)
# Check goodness of fit with Hosmer lemeshow
gof(modelBCLCA, plotROC = FALSE)$gof[1]
gof(modelBCLCB, plotROC = FALSE)$gof[1]
gof(modelBCLCC, plotROC = FALSE)$gof[1]
# Review influential observations with Cook's Distance
cooksD.BCLCA <- cooks.distance(modelBCLCA)
plot(cooksD.BCLCA, pch = "*", cex = 1.5)
abline(h = 4*mean(cooksD.BCLCA, na.rm = TRUE), col = "red")
influentialRows.BCLCA <- which(cooksD.BCLCA > 4*mean(cooksD.BCLCA, na.rm = TRUE))
dataBCLC %>%  # print influential observations
dplyr::select(one_of(names(modelBCLCA$model)[-1])) %>%
slice(influentialRows.BCLCA)
cooksD.BCLCB <- cooks.distance(modelBCLCB)
plot(cooksD.BCLCB, pch = "*", cex = 1.5)
abline(h = 4*mean(cooksD.BCLCB, na.rm = TRUE), col = "red")
influentialRows.BCLCB <- which(cooksD.BCLCB > 4*mean(cooksD.BCLCB, na.rm = TRUE))
dataBCLC %>%  # print influential observations
dplyr::select(one_of(names(modelBCLCB$model)[-1])) %>%
slice(influentialRows.BCLCB)
cooksD.BCLCC <- cooks.distance(modelBCLCC)
plot(cooksD.BCLCC, pch = "*", cex = 1.5)
abline(h = 4*mean(cooksD.BCLCC, na.rm = TRUE), col = "red")
influentialRows.BCLCC <- which(cooksD.BCLCC > 4*mean(cooksD.BCLCC, na.rm = TRUE))
dataBCLC %>%  # print influential observations
dplyr::select(one_of(names(modelBCLCC$model)[-1])) %>%
slice(influentialRows.BCLCC)
# Accuracy
probTrainBCLCA <- predict(modelBCLCA, type = "response")
modelBCLCA.cut <- optimalCutoff(dataBCLC$lives, probTrainBCLCA)
1 - misClassError(dataBCLC$lives, probTrainBCLCA, threshold = modelBCLCA.cut)
probTrainBCLCB <- predict(modelBCLCB, type = "response")
modelBCLCB.cut <- optimalCutoff(dataBCLC$lives, probTrainBCLCB)
1 - misClassError(dataBCLC$lives, probTrainBCLCB, threshold = modelBCLCB.cut)
probTrainBCLCC <- predict(modelBCLCC, type = "response")
modelBCLCC.cut <- optimalCutoff(dataBCLC$lives, probTrainBCLCC)
1 - misClassError(dataBCLC$lives, probTrainBCLCC, threshold = modelBCLCC.cut)
# AUC
gof(modelBCLCA)$auc
gof(modelBCLCB)$auc
gof(modelBCLCC)$auc
# PPV/NPV
precision(dataBCLC$lives, probTrainBCLCA, threshold = modelBCLCA.cut)
npv(dataBCLC$lives, probTrainBCLCA, threshold = modelBCLCA.cut)
precision(dataBCLC$lives, probTrainBCLCB, threshold = modelBCLCB.cut)
npv(dataBCLC$lives, probTrainBCLCB, threshold = modelBCLCB.cut)
precision(dataBCLC$lives, probTrainBCLCC, threshold = modelBCLCC.cut)
npv(dataBCLC$lives, probTrainBCLCC, threshold = modelBCLCC.cut)
# Best BCLC model
modelBCLC <- modelBCLCA
# BUILD THE BIOMARKER MODEL ----
# Correlation matrix for clinical variables
with(dataTrain, cor(afp, ggt, use = "pairwise.complete.obs"))
# Univariate logistic regressions
glmUnivarMarker <- cbind(varsMarker,
setNames(data.frame(matrix(nrow = length(varsMarker),
ncol = 3)),
c("OR_", "CI_", "pval")))
for (i in seq_along(varsMarker)) {
fit <- glm(as.formula(paste0("lives ~ ", varsMarker[i])),
data = dataTrain,
family = binomial)
beta <- coef(fit)
ci <- confint(fit)
pval <- coef(summary(fit))[2, 4]
glmUnivarMarker[i, 2] <- round(exp(beta[2]), 2)
glmUnivarMarker[i, 3] <- paste0("(", round(exp(ci[2,1]),2), " - ", round(exp(ci[2,2]),2), ")")
glmUnivarMarker[i, 4] <- round(pval,3)
}
print(glmUnivarMarker)
predMarker <- varsMarker
# Fit models
dataMarker <- dataTrain %>%
dplyr::select(one_of(predMarker, "lives"))
modelMarkerA <- glm(lives ~ afp,
data = dataMarker,
family = binomial)
modelMarkerB <- glm(lives ~ ggt,
data = dataMarker,
family = binomial)
modelMarkerC <- glm(lives ~ afp + ggt,
data = dataMarker,
family = binomial)
# Multicollinearity
vif(modelMarkerC)
# Autocorrelation
durbinWatsonTest(modelMarkerA)
durbinWatsonTest(modelMarkerB)
durbinWatsonTest(modelMarkerC)
# Check goodness of fit with Hosmer lemeshow
gof(modelMarkerA, plotROC = FALSE)$gof[1]
gof(modelMarkerB, plotROC = FALSE)$gof[1]
gof(modelMarkerC, plotROC = FALSE)$gof[1]
# Review influential observations with Cook's Distance
cooksD.MarkerA <- cooks.distance(modelMarkerA)
plot(cooksD.MarkerA, pch = "*", cex = 1.5)
abline(h = 4*mean(cooksD.MarkerA, na.rm = TRUE), col = "red")
influentialRows.MarkerA <- which(cooksD.MarkerA > 4*mean(cooksD.MarkerA, na.rm = TRUE))
dataMarker %>%  # print influential observations
dplyr::select(one_of(names(modelMarkerA$model)[-1])) %>%
slice(influentialRows.MarkerA)
cooksD.MarkerB <- cooks.distance(modelMarkerB)
plot(cooksD.MarkerB, pch = "*", cex = 1.5)
abline(h = 4*mean(cooksD.MarkerB, na.rm = TRUE), col = "red")
influentialRows.MarkerB <- which(cooksD.MarkerB > 4*mean(cooksD.MarkerB, na.rm = TRUE))
dataMarker %>%  # print influential observations
dplyr::select(one_of(names(modelMarkerB$model)[-1])) %>%
slice(influentialRows.MarkerB)
cooksD.MarkerC <- cooks.distance(modelMarkerC)
plot(cooksD.MarkerC, pch = "*", cex = 1.5)
abline(h = 4*mean(cooksD.MarkerC, na.rm = TRUE), col = "red")
influentialRows.MarkerC <- which(cooksD.MarkerB > 4*mean(cooksD.MarkerB, na.rm = TRUE))
dataMarker %>%  # print influential observations
dplyr::select(one_of(names(modelMarkerC$model)[-1])) %>%
slice(influentialRows.MarkerC)
# Accuracy
probTrainMarkerA <- predict(modelMarkerA, type = "response")
modelMarkerA.cut <- optimalCutoff(dataMarker$lives, probTrainMarkerA)
1 - misClassError(dataMarker$lives, probTrainMarkerA, threshold = modelMarkerA.cut)
probTrainMarkerB <- predict(modelMarkerB, type = "response")
modelMarkerB.cut <- optimalCutoff(dataMarker$lives, probTrainMarkerB)
1 - misClassError(dataMarker$lives, probTrainMarkerB, threshold = modelMarkerB.cut)
probTrainMarkerC <- predict(modelMarkerC, type = "response")
modelMarkerC.cut <- optimalCutoff(dataMarker$lives, probTrainMarkerC)
1 - misClassError(dataMarker$lives, probTrainMarkerC, threshold = modelMarkerC.cut)
# AUC
gof(modelMarkerA)$auc
gof(modelMarkerB)$auc
gof(modelMarkerC)$auc
# PPV/NPV
precision(dataMarker$lives, probTrainMarkerA, threshold = modelMarkerA.cut)
npv(dataMarker$lives, probTrainMarkerA, threshold = modelMarkerA.cut)
precision(dataMarker$lives, probTrainMarkerB, threshold = modelMarkerB.cut)
npv(dataMarker$lives, probTrainMarkerB, threshold = modelMarkerB.cut)
precision(dataMarker$lives, probTrainMarkerC, threshold = modelMarkerC.cut)
npv(dataMarker$lives, probTrainMarkerC, threshold = modelMarkerC.cut)
# Best Marker model
modelMarker <- modelMarkerA
# EVALUATE ADDITIVE VALUE OF TUMOR/LIVER & MOLECULAR BIOMARKER MODELS ----
# Recall individual models
modelClinical$formula
modelBCLC$formula
modelMarker$formula
# Build added models
probTrainClinical <- predict(modelClinical, type = "response")
modelCB <- glm(lives ~ age + diabetes + hypertension + symptoms + hemochromatosis + creatinine + leuk +
nodule_dim + hb + ascites + bilirubin_total,
data = dataTrain,
family = binomial)
probTrainCB <- predict(modelCB, type = "response")
summary(modelCB)
modelCBM <- glm(lives ~ age + diabetes + hypertension + symptoms + hemochromatosis + creatinine + leuk +
nodule_dim + hb + ascites + bilirubin_total +
afp,
data = dataTrain,
family = binomial)
probTrainCBM <- predict(modelCBM, type = "response")
summary(modelCBM)
# Likelihood ratio test
anova(modelClinical, modelCB, test ="Chisq")
anova(modelCB, modelCBM, test ="Chisq")
# Compare ROC curves
rocClinical <- roc(dataClinical$lives, probTrainClinical)
rocCB <- roc(dataTrain$lives, probTrainCB)
rocCBM <- roc(dataTrain$lives, probTrainCBM)
rocClinical$auc
rocCB$auc
rocCBM$auc
ggroc(data = list(C = rocClinical,
CB = rocCB,
CBM = rocCBM),
size = 1,
aes = c("color")) +
theme_light() +
ggthemes::scale_color_tableau(name = "Model")
roc.test(rocClinical, rocCB)
roc.test(rocCB, rocCBM)
roc.test(rocClinical, rocCBM)
# Accuracy
modelClinical.cut <- optimalCutoff(dataTrain$lives, probTrainClinical)
1 - misClassError(dataTrain$lives, probTrainClinical, threshold = modelClinical.cut)
modelCB.cut <- optimalCutoff(dataTrain$lives, probTrainCB)
1 - misClassError(dataTrain$lives, probTrainCB, threshold = modelCB.cut)
modelCBM.cut <- optimalCutoff(dataTrain$lives, probTrainCBM)
1 - misClassError(dataTrain$lives, probTrainCBM, threshold = modelCBM.cut)
# PPV/NPV
precision(dataTrain$lives, probTrainClinical, threshold = modelClinical.cut)
npv(dataTrain$lives, probTrainClinical, threshold = modelClinical.cut)
precision(dataTrain$lives, probTrainCB, threshold = modelCB.cut)
npv(dataTrain$lives, probTrainCB, threshold = modelCB.cut)
precision(dataTrain$lives, probTrainCBM, threshold = modelCBM.cut)
npv(dataTrain$lives, probTrainCBM, threshold = modelCBM.cut)
# INTERNALLY VALIDATE ON TEST DATA ----
# Make model predictions on test data
probTestClinical <- predict(modelClinical, type = "response", newdata = dataTest)
probTestCB <- predict(modelCB, type = "response", newdata = dataTest)
probTestCBM <- predict(modelCBM, type = "response", newdata = dataTest)
# Compare ROC curves
rocTestClinical <- roc(dataTest$lives, probTestClinical)
rocTestCB <- roc(dataTest$lives, probTestCB)
rocTestCBM <- roc(dataTest$lives, probTestCBM)
rocTestClinical$auc
rocTestCB$auc
rocTestCBM$auc
ggroc(data = list(C = rocTestClinical,
CB = rocTestCB,
CBM = rocTestCBM),
size = 1,
aes = c("color", "linetype")) +
theme_light() +
scale_linetype(name = "Model") +
ggthemes::scale_color_tableau(name = "Model")
roc.test(rocTestClinical, rocTestCB)
roc.test(rocTestCB, rocTestCBM)
roc.test(rocTestClinical, rocTestCBM)
# Accuracy
modelClinical.testCut <- optimalCutoff(dataTest$lives, probTestClinical)
1 - misClassError(dataTest$lives, probTestClinical, threshold = modelClinical.testCut)
modelCB.testCut <- optimalCutoff(dataTest$lives, probTestCB)
1 - misClassError(dataTest$lives, probTestCB, threshold = modelCB.testCut)
modelCBM.testCut <- optimalCutoff(dataTest$lives, probTestCBM)
1 - misClassError(dataTest$lives, probTestCBM, threshold = modelCBM.testCut)
# PPV/NPV
precision(dataTest$lives, probTestClinical, threshold = modelClinical.testCut)
npv(dataTest$lives, probTestClinical, threshold = modelClinical.testCut)
precision(dataTest$lives, probTestCB, threshold = modelCB.testCut)
npv(dataTest$lives, probTestCB, threshold = modelCB.testCut)
precision(dataTest$lives, probTestCBM, threshold = modelCBM.testCut)
npv(dataTest$lives, probTestCBM, threshold = modelCBM.testCut)
# K-FOLD CROSS VALIDATION ----
ctrl <- trainControl(method = "repeatedcv",
number = 10,
savePredictions = TRUE)
modelCV.C <- train(lives ~ diabetes + hypertension + symptoms + creatinine,
data = data.imp,
method = "glm",
family = "binomial",
trControl = ctrl,
tuneLength = 5)
modelCV.C$results
predictionsCV.C <- predict(modelCV.C, data.imp)
caret::confusionMatrix(predictionsCV.C, data.imp$lives)
modelCV.CB <- train(lives ~ diabetes + hypertension + symptoms + creatinine +
nodule_dim + inr + hb + ascites + alt + ast + alp + afp,
data = data.imp,
method = "glm",
family = "binomial",
trControl = ctrl,
tuneLength = 5)
modelCV.CB$results
predictionsCV.CB <- predict(modelCV.CB, data.imp)
caret::confusionMatrix(predictionsCV.CB, data.imp$lives)
# NOMOGRAM ----
# Nomogram function
## Create simple Fagan nomograms as ggplot objects
##   Based on Perl web-implementation (https://araw.mede.uic.edu/cgi-bin/testcalc.pl)
##   Authors: AM. Chekroud* & A. Schwartz (* adam dot chekroud at yale . edu)
##   December 2016
nomogrammer <- function(Prevalence,
Sens = NULL,
Spec = NULL,
Plr = NULL,
Nlr = NULL,
Detail = FALSE,
NullLine = FALSE,
LabelSize = (14/5),
Verbose = FALSE){
## Function inputs:
# Prevalence (prior probability) as a number between 0 and 1
# Either
# Sens & Spec
# model sensitivity and specificity as a number between 0 and 1
# Or
# Likelihood ratios
# Positive and Negative LRs (numeric)
## Function options:
# Detail: If true, will overlay key statistics onto the plot
# NullLine: If true, will add a line from prior prob through LR = 1
# LabelSize: Tweak this number to change the label sizes
# Verbose: Print out relevant metrics in the console
## Function returns:
# ggplot object
######################################
########## Libraries & Functions #####
######################################
## Libraries
require(ggplot2)
require(scales)
## Helper functions
##   (defined inside nomogrammer, so remain local only & wont clutter user env)
odds         <- function(p){
# Function converts probability into odds
o <- p/(1-p)
return(o)
}
logodds      <- function(p){
# Function returns logodds for a probability
lo <- log10(p/(1-p))
return(lo)
}
logodds_to_p <- function(lo){
# Function goes from logodds back to a probability
o <- 10^lo
p <- o/(1+o)
return(p)
}
p2percent <- function(p){
# Function turns numeric probability into string percentage
# e.g. 0.6346111 -> 63.5%
scales::percent(signif(p, digits = 3))}
######################################
########## Calculations     ##########
######################################
## Checking inputs
## Prevalence
# needs to exist
if(missing(Prevalence)){
stop("Prevalence is missing")
}
# needs to be numeric
if(!is.numeric(Prevalence)){stop("Prevalence should be numeric")}
# needs to be a prob not a percent
if((Prevalence > 1) | (Prevalence <= 0)){stop("Prevalence should be a probability (did you give a %?)")}
# Did user give sens & spec?
if(missing(Sens) | missing(Spec)){
sensspec <- FALSE
} else{ sensspec <- TRUE}
# if yes, make sure they are numbers
if(sensspec == TRUE){
if(!is.numeric(Sens)){stop("Sensitivity should be numeric")}
if(!is.numeric(Spec)){stop("Specificity should be numeric")}
# numbers that are probabilities not percentages
if((Sens > 1) | (Sens <= 0)){stop("Sensitivity should be a probability (did you give a %?)")}
if((Spec > 1) | (Spec <= 0)){stop("Specificity should be a probability (did you give a %?)")}
}
# Did user give PLR & NLR?
if(missing(Plr) | missing(Nlr)){
plrnlr <- FALSE
} else{plrnlr <- TRUE}
# if yes, make sure they are numbers
if(plrnlr == TRUE){
if(!is.numeric(Plr)){stop("PLR should be numeric")}
if(!is.numeric(Nlr)){stop("NLR should be numeric")}
# numbers that vaguely make sense
if(Plr < 1){stop("PLR shouldn't be less than 1")}
if(Nlr < 0){stop("NLR shouldn't be below zero")}
if(Nlr > 1){stop("NLR shouldn't be more than 1")}
}
# Did they give a valid sensspec and plrnlr? If yes, ignore the LRs and tell them
if((sensspec == TRUE) && (plrnlr == TRUE) ){
warning("You provided sens/spec as well as likelihood ratios-- I ignored the LRs!")
}
## If sens/spec provided, we calculate posterior probabilities & odds using sens & spec
##  otherwise, if plr and nlr provided, we calculate posteriors using them
##  if neither exist, then return an error
if(sensspec == TRUE){
prior_prob  <- Prevalence
prior_odds  <- odds(prior_prob)
sensitivity <- Sens
specificity <- Spec
PLR <- sensitivity/(1-specificity)
NLR <- (1-sensitivity)/specificity
post_odds_pos  <- prior_odds * PLR
post_odds_neg  <- prior_odds * NLR
post_prob_pos  <- post_odds_pos/(1+post_odds_pos)
post_prob_neg  <- post_odds_neg/(1+post_odds_neg)
} else if(plrnlr == TRUE){
prior_prob  <- Prevalence
prior_odds  <- odds(prior_prob)
PLR <- Plr
NLR <- Nlr
sensitivity <- (PLR*(1-NLR))/(PLR-NLR)    ## TODO: check Adam's math!
specificity <- (1-PLR)/(NLR-PLR)          ## TODO: check Adam's math!
post_odds_pos  <- prior_odds * PLR
post_odds_neg  <- prior_odds * NLR
post_prob_pos  <- post_odds_pos/(1+post_odds_pos)
post_prob_neg  <- post_odds_neg/(1+post_odds_neg)
} else{
stop("Couldn't find sens & spec, or positive & negative likelihood ratios")
}
######################################
########## Plotting (prep)  ##########
######################################
## Set common theme preferences up front
theme_set(theme_bw() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_text(angle = 0),
axis.title.y.right = element_text(angle = 0),
axis.line = element_blank(),
panel.grid = element_blank(),
legend.position = "none"
)
)
## Setting up the points of interest along the y-axes
# Select probabilities of interest (nb as percentages)
ticks_prob    <- c(0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 30,
40, 50, 60, 70, 80, 90, 95, 99)
# Convert % to odds
ticks_odds    <- odds(ticks_prob/100)
# Convert % to logodds
ticks_logodds <- logodds(ticks_prob/100)
# Select the likelihood ratios of interest (for the middle y-axis)
ticks_lrs     <- sort(c(10^(-3:3), 2*(10^(-3:2)), 5*(10^(-3:2))))
# Log10 them since plot is in logodds space
ticks_log_lrs <- log10(ticks_lrs)
## Fixing particular x-coordinates
left     <- 0
right    <- 1
middle   <- 0.5
midright <- 0.75
## Lay out the four key plot points
##  (the start and finish of the positive and negative lines)
# Initially these are expressed as probabilities
df <- data.frame(x=c(left, right, left, right),
y=c(prior_prob, post_prob_pos, prior_prob, post_prob_neg),
line = c("pos", "pos", "neg", "neg"))
adj_min      <- range(ticks_logodds)[1]
adj_max      <- range(ticks_logodds)[2]
adj_diff     <- adj_max - adj_min
scale_factor <- abs(adj_min) - adj_diff/2
#df$lo_y <- ifelse(df$x==left,(10/adj_diff)*logodds(1-df$y)-1,logodds(df$y))
# Convert probabilities to logodds for plotting
df$lo_y  <- ifelse(df$x==left,logodds(1-df$y)-scale_factor,logodds(df$y))
# zero         <- data.frame(x = c(left,right),
#                            y = c(0,0),
#                            line = c('pos','pos'),
#                            lo_y = c(-scale_factor,0))
rescale   <- range(ticks_logodds) + abs(adj_min) - adj_diff/2
rescale_x_breaks  <- ticks_logodds + abs(adj_min) - adj_diff/2
######################################
########## Plot             ##########
######################################
p <- ggplot(df) +
geom_line(aes(x = x, y = lo_y, color = line), size = 1) +
geom_vline(xintercept = middle) +
annotate(geom = "text",
x = rep(middle+.075, length(ticks_log_lrs)),
y = (ticks_log_lrs-scale_factor)/2,
label = ticks_lrs,
size = rel(LabelSize)) +
annotate(geom="point",
x = rep(middle, length(ticks_log_lrs)),
y = (ticks_log_lrs-scale_factor)/2,
size = 1) +
scale_x_continuous(expand = c(0,0)) +
scale_y_continuous(expand = c(0,0),
limits = rescale,
breaks = -rescale_x_breaks,
labels = ticks_prob,
name = "prior \n prob.",
sec.axis = sec_axis(trans = ~.,
name = "posterior \n prob.",
labels = ticks_prob,
breaks = ticks_logodds))
## Optional overlay text: prevalence, PLR/NLR, and posterior probabilities
detailedAnnotation <- paste(
paste0("prevalence = ", p2percent(prior_prob)),
paste("PLR =", signif(PLR, 3),", NLR =", signif(NLR, 3)),
paste("post. pos =", p2percent(post_prob_pos),
", neg =", p2percent(post_prob_neg)),
sep = "\n")
## Optional amendments to the plot
## Do we add the null line i.e. LR = 1, illustrating an uninformative model
if(NullLine == TRUE){
## If yes, first calculate the start and end points
uninformative <- data.frame(
x = c(left,right),
lo_y = c( (logodds(1-prior_prob) - scale_factor) , logodds(prior_prob))
)
p <- p + geom_line(aes(x = x, y = lo_y), data = uninformative,
color = "gray",
lty = 2,
inherit.aes = FALSE)
}
## Do we add the detailed stats to the top right?
if(Detail == TRUE){
p <- p + annotate(geom = "text",
x = midright,
y = 2,
label = detailedAnnotation,
size = rel(LabelSize))
}
if(Verbose == TRUE){
writeLines(
text = c(
paste0("prevalence = ", p2percent(prior_prob)),
paste("PLR =", signif(PLR, 3)),
paste("NLR =", signif(NLR, 3)),
paste("posterior probability (positive) =", p2percent(post_prob_pos)),
paste("posterior probability (negative) =", p2percent(post_prob_neg)),
paste("sensitivity =", p2percent(sensitivity)),
paste("specificity =", p2percent(specificity))
# sep = "\n"
)
)
}
return(p)
}
nomogrammer(Prevalence = prop.table(table(data.imp$lives))[2],
Sens = 0.5079, Spec = 0.7755)
nomogrammer(Prevalence = prop.table(table(data.imp$lives))[2],
Sens = 0.7143, Spec = 0.8878)
